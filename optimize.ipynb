{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"JDK 8\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[*]\") \\\n",
    "      .appName(\"PySparkApplication\") \\\n",
    "      .getOrCreate()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring the schema -- BAD because the whole file needs to be read after loading and before transforming to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [order_id#17,customer_id#18,product_name#19,category#20,quantity#21,unit_price#22,order_date#23,sales_rep#24,region#25] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/JoseIungo/Desktop/Github/iungo/product/Optimizing Data ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,customer_id:string,product_name:string,category:string,quantity:string,uni...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the schema explicit causing a 3.2 second increase in the speed of reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"order_id\", IntegerType(), True),\n",
    "                     StructField(\"customer_id\", StringType(), True),\n",
    "                     StructField(\"product_name\", StringType(), True),\n",
    "                     StructField(\"category\", StringType(), True),\n",
    "                     StructField(\"quantity\", IntegerType(), True),\n",
    "                     StructField(\"unit_price\", DoubleType(), True),\n",
    "                     StructField(\"order_date\", DateType(), True),\n",
    "                     StructField(\"sales_rep\", StringType(), True),\n",
    "                     StructField(\"region\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.read.schema(schema).option(\"header\", True).csv(\"sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [order_id#35,customer_id#36,product_name#37,category#38,quantity#39,unit_price#40,order_date#41,sales_rep#42,region#43] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/JoseIungo/Desktop/Github/iungo/product/Optimizing Data ..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:int,customer_id:string,product_name:string,category:string,quantity:int,unit_pric...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = new_df.groupBy(\"region\", \"category\").agg(F.sum(\"quantity\").alias(\"total_quantity\"), F.avg(\"unit_price\").alias(\"avg_price\")).filter(F.col(\"region\") == \"North America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[region#8, category#3], functions=[sum(quantity#4), avg(unit_price#5)])\n",
      "   +- Exchange hashpartitioning(region#8, category#3, 200), ENSURE_REQUIREMENTS, [plan_id=17]\n",
      "      +- HashAggregate(keys=[region#8, category#3], functions=[partial_sum(quantity#4), partial_avg(unit_price#5)])\n",
      "         +- Filter (isnotnull(region#8) AND (region#8 = North America))\n",
      "            +- FileScan csv [category#3,quantity#4,unit_price#5,region#8] Batched: false, DataFilters: [isnotnull(region#8), (region#8 = North America)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/JoseIungo/Desktop/Github/iungo/product/Optimizing Data ..., PartitionFilters: [], PushedFilters: [IsNotNull(region), EqualTo(region,North America)], ReadSchema: struct<category:string,quantity:int,unit_price:double,region:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------+-----------------+\n",
      "|       region|   category|total_quantity|        avg_price|\n",
      "+-------------+-----------+--------------+-----------------+\n",
      "|North America| Appliances|             2|           189.99|\n",
      "|North America|Electronics|             4|949.9900000000001|\n",
      "|North America|  Furniture|             2|           699.99|\n",
      "+-------------+-----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = new_df.filter(F.col(\"region\") == \"North America\").groupBy(\"region\", \"category\").agg(F.sum(\"quantity\").alias(\"total_quantity\"), F.avg(\"unit_price\").alias(\"avg_price\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[region#8, category#3], functions=[sum(quantity#4), avg(unit_price#5)])\n",
      "   +- Exchange hashpartitioning(region#8, category#3, 200), ENSURE_REQUIREMENTS, [plan_id=180]\n",
      "      +- HashAggregate(keys=[region#8, category#3], functions=[partial_sum(quantity#4), partial_avg(unit_price#5)])\n",
      "         +- Filter (isnotnull(region#8) AND (region#8 = North America))\n",
      "            +- FileScan csv [category#3,quantity#4,unit_price#5,region#8] Batched: false, DataFilters: [isnotnull(region#8), (region#8 = North America)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/JoseIungo/Desktop/Github/iungo/product/Optimizing Data ..., PartitionFilters: [], PushedFilters: [IsNotNull(region), EqualTo(region,North America)], ReadSchema: struct<category:string,quantity:int,unit_price:double,region:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Evaluation#\n",
    "\n",
    "Lazy evaluation means that we do not do any work until it is necessary.\n",
    "\n",
    "Some functions that force evaluation:\n",
    "\n",
    "df.show()\n",
    "df.count()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------+-----------------+\n",
      "|       region|   category|total_quantity|        avg_price|\n",
      "+-------------+-----------+--------------+-----------------+\n",
      "|North America| Appliances|             2|           189.99|\n",
      "|North America|Electronics|             4|949.9900000000001|\n",
      "|North America|  Furniture|             2|           699.99|\n",
      "+-------------+-----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU COUNT\n",
    "Spark uses all available CPU's when directed to with the opening argument local[*]. However if you need to limit the amount you can use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "num = os.cpu_count()\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[region: string, category: string, total_quantity: bigint, avg_price: double]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query.repartition(math.floor(num / 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------+-----------------+\n",
      "|       region|   category|total_quantity|        avg_price|\n",
      "+-------------+-----------+--------------+-----------------+\n",
      "|North America| Appliances|             2|           189.99|\n",
      "|North America|Electronics|             4|949.9900000000001|\n",
      "|North America|  Furniture|             2|           699.99|\n",
      "+-------------+-----------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a dimension table to broadcast joins \n",
    "\n",
    "When you are joining tables in a distributed context, it is better to broadcast the join axcross the network so that each node gets its own copy of the table so as to avoid unneccessary shuffling (where data is moved across nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [(\"North America\", \"NA\"), (\"Europe\", \"EU\"), (\"Asia Pacific\", \"AP\")]\n",
    "regions_df = spark.createDataFrame(regions, [\"region\", \"region_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = new_df.join(regions_df, \"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-----------+-------------------+-----------+--------+----------+----------+-------------+-----------+\n",
      "|       region|order_id|customer_id|       product_name|   category|quantity|unit_price|order_date|    sales_rep|region_code|\n",
      "+-------------+--------+-----------+-------------------+-----------+--------+----------+----------+-------------+-----------+\n",
      "|North America|    1017|       C006|          Microwave| Appliances|       1|    179.99|2024-02-01| David Wilson|         NA|\n",
      "|North America|    1016|       C011|       Dining Table|  Furniture|       1|    799.99|2024-01-30|Alice Johnson|         NA|\n",
      "|North America|    1011|       C008|            Blender| Appliances|       1|    199.99|2024-01-25| David Wilson|         NA|\n",
      "|North America|    1010|       C003|         Tablet Pro|Electronics|       1|    649.99|2024-01-24|Alice Johnson|         NA|\n",
      "|North America|    1005|       C004|       Smartphone X|Electronics|       1|    899.99|2024-01-19| David Wilson|         NA|\n",
      "|North America|    1004|       C001|      Standing Desk|  Furniture|       1|    599.99|2024-01-18|Alice Johnson|         NA|\n",
      "|North America|    1001|       C001|      Laptop Pro 15|Electronics|       2|   1299.99|2024-01-15|Alice Johnson|         NA|\n",
      "|       Europe|    1020|       C013|        Smart Watch|Electronics|       1|    399.99|2024-02-04|    Bob Smith|         EU|\n",
      "|       Europe|    1018|       C012|   Wireless Speaker|Electronics|       2|    129.99|2024-02-02|    Eve Brown|         EU|\n",
      "|       Europe|    1014|       C010|       Air Purifier| Appliances|       1|    299.99|2024-01-28|    Bob Smith|         EU|\n",
      "|       Europe|    1012|       C004|          Bookshelf|  Furniture|       2|    179.99|2024-01-26|    Eve Brown|         EU|\n",
      "|       Europe|    1008|       C002|     Monitor 27inch|Electronics|       2|    399.99|2024-01-22|    Bob Smith|         EU|\n",
      "|       Europe|    1006|       C005|       Coffee Maker| Appliances|       2|    149.99|2024-01-20|    Eve Brown|         EU|\n",
      "|       Europe|    1002|       C002|Wireless Headphones|Electronics|       1|    199.99|2024-01-16|    Bob Smith|         EU|\n",
      "| Asia Pacific|    1019|       C007|       File Cabinet|  Furniture|       1|    249.99|2024-02-03| Frank Miller|         AP|\n",
      "| Asia Pacific|    1015|       C005|       Gaming Mouse|Electronics|       2|     59.99|2024-01-29|  Carol Davis|         AP|\n",
      "| Asia Pacific|    1013|       C009|          Webcam HD|Electronics|       3|     99.99|2024-01-27| Frank Miller|         AP|\n",
      "| Asia Pacific|    1009|       C007|          Desk Lamp|  Furniture|       1|     89.99|2024-01-23|  Carol Davis|         AP|\n",
      "| Asia Pacific|    1007|       C006| Ergonomic Keyboard|Electronics|       4|     79.99|2024-01-21| Frank Miller|         AP|\n",
      "| Asia Pacific|    1003|       C003|       Office Chair|  Furniture|       3|    299.99|2024-01-17|  Carol Davis|         AP|\n",
      "+-------------+--------+-----------+-------------------+-----------+--------+----------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "broad_join = new_df.join(F.broadcast(regions_df), \"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+-----------+-------------------+-----------+--------+----------+----------+-------------+-----------+\n",
      "|       region|order_id|customer_id|       product_name|   category|quantity|unit_price|order_date|    sales_rep|region_code|\n",
      "+-------------+--------+-----------+-------------------+-----------+--------+----------+----------+-------------+-----------+\n",
      "|North America|    1001|       C001|      Laptop Pro 15|Electronics|       2|   1299.99|2024-01-15|Alice Johnson|         NA|\n",
      "|       Europe|    1002|       C002|Wireless Headphones|Electronics|       1|    199.99|2024-01-16|    Bob Smith|         EU|\n",
      "| Asia Pacific|    1003|       C003|       Office Chair|  Furniture|       3|    299.99|2024-01-17|  Carol Davis|         AP|\n",
      "|North America|    1004|       C001|      Standing Desk|  Furniture|       1|    599.99|2024-01-18|Alice Johnson|         NA|\n",
      "|North America|    1005|       C004|       Smartphone X|Electronics|       1|    899.99|2024-01-19| David Wilson|         NA|\n",
      "|       Europe|    1006|       C005|       Coffee Maker| Appliances|       2|    149.99|2024-01-20|    Eve Brown|         EU|\n",
      "| Asia Pacific|    1007|       C006| Ergonomic Keyboard|Electronics|       4|     79.99|2024-01-21| Frank Miller|         AP|\n",
      "|       Europe|    1008|       C002|     Monitor 27inch|Electronics|       2|    399.99|2024-01-22|    Bob Smith|         EU|\n",
      "| Asia Pacific|    1009|       C007|          Desk Lamp|  Furniture|       1|     89.99|2024-01-23|  Carol Davis|         AP|\n",
      "|North America|    1010|       C003|         Tablet Pro|Electronics|       1|    649.99|2024-01-24|Alice Johnson|         NA|\n",
      "|North America|    1011|       C008|            Blender| Appliances|       1|    199.99|2024-01-25| David Wilson|         NA|\n",
      "|       Europe|    1012|       C004|          Bookshelf|  Furniture|       2|    179.99|2024-01-26|    Eve Brown|         EU|\n",
      "| Asia Pacific|    1013|       C009|          Webcam HD|Electronics|       3|     99.99|2024-01-27| Frank Miller|         AP|\n",
      "|       Europe|    1014|       C010|       Air Purifier| Appliances|       1|    299.99|2024-01-28|    Bob Smith|         EU|\n",
      "| Asia Pacific|    1015|       C005|       Gaming Mouse|Electronics|       2|     59.99|2024-01-29|  Carol Davis|         AP|\n",
      "|North America|    1016|       C011|       Dining Table|  Furniture|       1|    799.99|2024-01-30|Alice Johnson|         NA|\n",
      "|North America|    1017|       C006|          Microwave| Appliances|       1|    179.99|2024-02-01| David Wilson|         NA|\n",
      "|       Europe|    1018|       C012|   Wireless Speaker|Electronics|       2|    129.99|2024-02-02|    Eve Brown|         EU|\n",
      "| Asia Pacific|    1019|       C007|       File Cabinet|  Furniture|       1|    249.99|2024-02-03| Frank Miller|         AP|\n",
      "|       Europe|    1020|       C013|        Smart Watch|Electronics|       1|    399.99|2024-02-04|    Bob Smith|         EU|\n",
      "+-------------+--------+-----------+-------------------+-----------+--------+----------+----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broad_join.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
